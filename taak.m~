%% overhead file for control and calling smaller functions

%first step load data
data= openFilesFromDir('Drink_glass');
nondata = openFilesNotFromDir('Drink_glass');

%% exercise 1

%1.1: preprocessing
[trainingTrue,validationTrue,testingTrue]= divideAndConquer(data);
[trainingFalse,validationFalse,testingFalse]= divideAndConquer(nondata);

%1.2: feature extraction -> this is for one instance in the subfolder, we
%need to evaluate ALL of the files
%gravity = getGravity(training{1}); %1.2.1
[grav] = getGravity(trainingTrue{1}); %1.2.1
ampdata = GetAmplitudeOfAcceleration(trainingTrue{1}); %1.2.2 we only need this for intermediate results
nodc = RemoveDCComponent(trainingTrue{1}); %1.2.3
[stdev,skew] = timeDomainFeatures(trainingTrue{1}); %1.2.4
[f25, f75] = getFrequencyDomain(trainingTrue{1}); %1.2.5


%features we should have now:
%x=[gx,gy,gz,stdev,skewness, F25, F75]
X = getFeatures(trainingTrue{2}); % example for testing

trainingTrue = cellfun(@getFeatures,trainingTrue,'UniformOutput',0);
trainingTrue = vertcat(trainingTrue{:});
trainingFalse = cellfun(@getFeatures,trainingFalse,'UniformOutput',0);
trainingFalse = vertcat(trainingFalse{:});
trainingX = [trainingTrue; trainingFalse];
validationTrue = cellfun(@getFeatures,validationTrue,'UniformOutput',0);
validationTrue = vertcat(validationTrue{:});
validationFalse = cellfun(@getFeatures,validationFalse,'UniformOutput',0);
validationFalse = vertcat(validationFalse{:});
validationX = [validationTrue; validationFalse];
testingTrue = cellfun(@getFeatures,testingTrue,'UniformOutput',0);
testingTrue = vertcat(testingTrue{:});
testingFalse = cellfun(@getFeatures,testingFalse,'UniformOutput',0);
testingFalse = vertcat(testingFalse{:});
testingX = [testingTrue; testingFalse];

trainingY = [ones(length(trainingTrue),1); zeros(length(trainingFalse),1)];
validationY = [ones(length(validationTrue),1); zeros(length(validationFalse),1)];
testingY = [ones(length(testingTrue),1); zeros(length(testingFalse),1)];

%1.3 feature selection: 

% create feature vector -> for training data set 
singleX = cellfun(@getFeatures,data,'UniformOutput',0); % extract features from data 
singleX = vertcat(singleX{:}); % convert ugly resulting cellArray to beautiful useful feature Array/vector

% creating a group vector
G = strings(size(singleX,1),1);
G(:,1) = 'drink glass';

X2=cellfun(@getFeatures,nondata,'UniformOutput',0); % extract features from nondata 
X2 = vertcat(X2{:}); % convert ugly resulting cellArray to beautiful useful feature Array/vector

G2 = strings(size(X2,1),1);
G2(:,1) = 'not drink glass';

X= [singleX;X2];
G= [G;G2];

% now we need other samples as well
varNames = {'gx'; 'gy'; 'gz'; 'stdev'; 'skewness'; 'F25'; 'F75'};

% plot all features
close all,figure;
%gplotmatrix(X,[],G,['b' 'r'],[],[],'on',[],varNames,varNames);

% best features -> stdev & F25


%% exercise 2 
%2.1 cost function and gradient
% preparing input data and classifier labels
selectedTrainingDataX = trainingX(:, [1,4]);
[m, n] = size(selectedTrainingDataX);
selectedTrainingDataX = [ones(m, 1), selectedTrainingDataX];

initial_theta = zeros(n + 1, 1);
lambda = 0;

[cost, grad] = costFunctionReg(initial_theta, selectedTrainingDataX, trainingY, lambda);
fprintf('Cost at initial theta (zeros): %f\n', cost);
fprintf('Gradient at initial theta (zeros): \n');
fprintf(' %f \n', grad);

options = optimset('GradObj', 'on', 'MaxIter', 400);
[theta, cost, exit_flag] = fminunc(@(t)(costFunctionReg(t, selectedTrainingDataX, trainingY, 0)), initial_theta, options);

fprintf('Cost at theta found by fminunc: %f\n', cost);
fprintf('theta: \n');
fprintf(' %f \n', theta);

% Plot Boundary
plotDecisionBoundary(theta, selectedTrainingDataX, trainingY);


%2.2 linear model with 2 features

%trainingFeatures = cellfun(@getFeatures,training,'UniformOutput',0);
%trainingFeatures = vertcat(trainingFeatures{:});

%trainingX= trainingFeatures(:,[4,6]), trainingY= ones(size(trainingX,1),1);

%  Set options for fminunc
%options = optimset('GradObj', 'on', 'MaxIter', 400);

%  Run fminunc to obtain the optimal theta
%  This function will return theta and the cost 
%[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);

% Print theta to screen
%fprintf('Cost at theta found by fminunc: %f\n', cost);
%fprintf('theta: \n');
%fprintf(' %f \n', theta);

% Plot Boundary
%plotDecisionBoundary(theta, X, y);

% Chiel


%2.3 polynomial features from 2 features
selectedTrainingDataX = trainingX(:, [2,4]);
selectedValidationDataX = validationX(:, [2,4]);
selectedTrainingDataX = mapFeature(selectedTrainingDataX(:, 1), selectedTrainingDataX(:, 2));
selectedValidationDataX = mapFeature(selectedValidationDataX(:, 1), selectedValidationDataX(:, 2));

[bestTheta, bestLambda, bestAccuracy, lambdaVSTrainingAndValidation] = getBestLambda( selectedTrainingDataX, selectedValidationDataX, trainingY, validationY);

loglog(lambdaVSTrainingAndValidation(:,1),lambdaVSTrainingAndValidation(:,2),lambdaVSTrainingAndValidation(:,1),lambdaVSTrainingAndValidation(:,3))
plotDecisionBoundary(bestTheta, selectedTrainingDataX, trainingY);

%fprintf('Cost at theta found by fminunc: %f\n', cost);
%fprintf('theta: \n');
%fprintf(' %f \n', theta);

%plotDecisionBoundary(theta, selectedTrainingDataX, trainingY);

%2.4 linear classifier with 7 features
selectedTrainingDataX = trainingX;
selectedValidationDataX = validationX;

[bestTheta, bestLambda, bestAccuracy, lambdaVSTrainingAndValidation] = getBestLambda( selectedTrainingDataX, selectedValidationDataX, trainingY, validationY);

loglog(lambdaVSTrainingAndValidation(:,1),lambdaVSTrainingAndValidation(:,2),lambdaVSTrainingAndValidation(:,1),lambdaVSTrainingAndValidation(:,3))

%2.5 non-linear classifier with 7 features
selectedTrainingDataX = trainingX;
selectedValidationDataX = validationX;
selectedTrainingDataX = x2fx(selectedTrainingDataX,'quadratic');
selectedValidationDataX = x2fx(selectedValidationDataX,'quadratic');

[bestTheta, bestLambda, bestAccuracy, lambdaVSTrainingAndValidation] = getBestLambda( selectedTrainingDataX, selectedValidationDataX, trainingY, validationY);

size(selectedTrainingDataX)
loglog(lambdaVSTrainingAndValidation(:,1),lambdaVSTrainingAndValidation(:,2),lambdaVSTrainingAndValidation(:,1),lambdaVSTrainingAndValidation(:,3))



